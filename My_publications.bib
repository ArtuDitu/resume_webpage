
@article{czeszumskiHyperscanningValidMethod2020a,
  title = {Hyperscanning: {{A Valid Method}} to {{Study Neural Inter}}-Brain {{Underpinnings}} of {{Social Interaction}}},
  shorttitle = {Hyperscanning},
  author = {Czeszumski, Artur and Eustergerling, Sara and Lang, Anne and Menrath, David and Gerstenberger, Michael and Schuberth, Susanne and Schreiber, Felix and Rendon, Zadkiel Zuluaga and K{\"o}nig, Peter},
  year = {2020},
  month = feb,
  volume = {14},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2020.00039},
  abstract = {Social interactions are a crucial part of human life. Understanding the neural underpinnings of social interactions is a challenging task that the hyperscanning method has been trying to tackle over the last two decades. Here, we review the existing literature and evaluate the current state of the hyperscanning method. We review the type of methods (fMRI, M/EEG, and fNIRS) that are used to measure brain activity from more than one participant simultaneously and weigh their pros and cons for hyperscanning. Further, we discuss different types of analyses that are used to estimate brain networks and synchronization. Lastly, we present results of hyperscanning studies in the context of different cognitive functions and their relations to social interactions. All in all, we aim to comprehensively present methods, analyses, and results from the last 20 years of hyperscanning research.},
  file = {files/941/Czeszumski et al. - 2020 - Hyperscanning A Valid Method to Study Neural Inte.pdf},
  journal = {Frontiers in Human Neuroscience},
  language = {en}
}

@techreport{czeszumskiLetMeMake2020,
  title = {Let {{Me Make You Happy}}, {{And I}}'ll {{Tell You How}} You Look around: {{Using}} an {{Approach}}-{{Avoidance Task}} as an {{Embodied Emotion Prime}} in a {{Free}}-{{Viewing Task}}},
  shorttitle = {Let {{Me Make You Happy}}, {{And I}}'ll {{Tell You How}} You Look around},
  author = {Czeszumski, Artur and Albers, Friederike and Walter, Sven and K{\"o}nig, Peter},
  year = {2020},
  month = sep,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.09.09.289249},
  abstract = {The embodied approach of human cognition suggests that concepts are deeply dependent upon and constrained by an agent's physical body's characteristics, such as performed body movements. In this study, we attempted to broaden previous research on emotional priming, investigating the interaction of emotions and visual exploration. We used the joystick-based approach-avoidance task to influence the emotional states of participants, and subsequently, we presented pictures of news web pages on a computer screen and measured participant's eye movements. As a result, the number of fixations on images increased, the total dwell time increased, and the average saccade length from outside of the images towards the images decreased after the bodily congruent priming phase. The combination of these effects suggests increased attention to web pages' image content after the participants performed bodily congruent actions in the priming phase. Thus, congruent bodily interaction with images in the priming phase fosters visual interaction in the subsequent exploration phase.},
  file = {files/949/Czeszumski et al. - 2020 - Let Me Make You Happy, And Iâ€™ll Tell You How you l.pdf},
  language = {en},
  type = {Preprint}
}

@article{czeszumskiSocialSituationAffects2019a,
  title = {The {{Social Situation Affects How We Process Feedback About Our Actions}}},
  author = {Czeszumski, Artur and Ehinger, Benedikt V. and Wahn, Basil and K{\"o}nig, Peter},
  year = {2019},
  month = feb,
  volume = {10},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.00361},
  abstract = {Humans achieve their goals in joint action tasks either by cooperation or competition. In the present study, we investigated the neural processes underpinning error and monetary rewards processing in such cooperative and competitive situations. We used electroencephalography (EEG) and analyzed event-related potentials (ERPs) triggered by feedback in both social situations. 26 dyads performed a joint four-alternative forced choice (4AFC) visual task either cooperatively or competitively. At the end of each trial, participants received performance feedback about their individual and joint errors and accompanying monetary rewards. Furthermore, the outcome, i.e., resulting positive, negative, or neutral rewards, was dependent on the pay-off matrix, defining the social situation either as cooperative or competitive. We used linear mixed effects models to analyze the feedback-related-negativity (FRN) and used the Threshold-free cluster enhancement (TFCE) method to explore activations of all electrodes and times. We found main effects of the outcome and social situation, but no interaction at mid-line frontal electrodes. The FRN was more negative for losses than wins in both social situations. However, the FRN amplitudes differed between social situations. Moreover, we compared monetary with neutral outcomes in both social situations. Our exploratory TFCE analysis revealed that processing of feedback differs between cooperative and competitive situations at right temporo-parietal electrodes where the cooperative situation elicited more positive amplitudes. Further, the differences induced by the social situations were stronger in participants with higher scores on a perspective taking test. In sum, our results replicate previous studies about the FRN and extend them by comparing neurophysiological responses to positive and negative outcomes in a task that simultaneously engages two participants in competitive and cooperative situations.},
  file = {files/946/Czeszumski et al. - 2019 - The Social Situation Affects How We Process Feedba.pdf},
  journal = {Frontiers in Psychology},
  language = {en}
}

@techreport{pavlovEEGManyLabsInvestigatingReplicability2020,
  title = {\#{{EEGManyLabs}}: {{Investigating}} the {{Replicability}} of {{Influential EEG Experiments}}},
  shorttitle = {\#{{EEGManyLabs}}},
  author = {Pavlov, Yuri G. and Adamian, Nika and Appelhoff, Stefan and Arvaneh, Mahnaz and Benwell, Christopher and Beste, Christian and Bland, Amy and Bradford, Daniel E. and Bublatzky, Florian and Busch, Niko and Clayson, Peter E and Cruse, Damian and Czeszumski, Artur and Dreber, Anna and Dumas, Guillaume and Ehinger, Benedikt Valerian and Ganis, Giorgio and He, Xun and Hinojosa, Jos{\'e} Antonio and {Huber-Huber}, Christoph and Inzlicht, Michael and Jack, Bradley N and Johannesson, Magnus and Jones, Rhiannon and Kalenkovich, Evgenii and Kaltwasser, Laura and {Karimi-Rouzbahani}, Hamid and K{\"o}nig, Peter and Kouara, Layla and Kulke, Louisa and Ladouceur, Cecile and Langer, Nicolas and Liesefeld, Heinrich Ren{\'e} and Luque, David and MacNamara, Annmarie and Muthuraman, Muthuraman and Neal, Lauren Browning and Nilsonne, Gustav and Niso, Guiomar and Ocklenburg, Sebastian and Oostenveld, Robert and Pernet, Cyril R and Pourtois, Gilles and Ruzzoli, Manuela and Sass, Sarah and Schaefer, Alexandre and Senderecka, Magdalena and Snyder, Joel S. and Tamnes, Christian K. and Tognoli, Emmanuelle and {van Vugt}, Marieke K. and Verona, Edelyn and Vloeberghs, Robin and Welke, Dominik and Wessel, Jan and Zakharov, Ilya and Mushtaq, Faisal},
  year = {2020},
  month = nov,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/528nr},
  abstract = {There is growing awareness across the neuroscience community that the replicability of findings on the relationship between brain activity and cognitive phenomena can be improved by conducting studies with high statistical power that adhere to well-defined and standardized analysis pipelines. Inspired by efforts from the psychological sciences, and with the desire to examine some of the foundational findings using electroencephalography (EEG), we have launched \#EEGManyLabs, a large-scale international collaborative replication effort. Since its discovery in the early 20th century, EEG has had a profound influence on our understanding of human cognition, but there is limited evidence on the replicability of some of the most highly cited discoveries. After a systematic search and selection process, we have identified 27 of the most influential and continually cited studies in the field. We plan to directly test the replicability of key findings from 20 of these studies in teams of at least three independent laboratories. The design and protocol of each replication effort will be submitted as a Registered Report and peer-reviewed prior to data collection. Prediction markets, open to all EEG researchers, will be used as a forecasting tool to examine which findings the community expects to replicate. This project will update our confidence in some of the most influential EEG findings and generate a large open access database that can be used to inform future research practices. Finally, through this international effort, we hope to create a cultural shift towards inclusive, high-powered multi-laboratory collaborations.},
  file = {files/947/Pavlov et al. - 2020 - #EEGManyLabs Investigating the Replicability of I.pdf},
  language = {en},
  type = {Preprint}
}

@article{wahnDyadicTriadicSearch2020,
  title = {Dyadic and Triadic Search: {{Benefits}}, Costs, and Predictors of Group Performance},
  shorttitle = {Dyadic and Triadic Search},
  author = {Wahn, Basil and Czeszumski, Artur and Labusch, Melanie and Kingstone, Alan and K{\"o}nig, Peter},
  year = {2020},
  month = jul,
  volume = {82},
  pages = {2415--2433},
  issn = {1943-3921, 1943-393X},
  doi = {10.3758/s13414-019-01915-0},
  abstract = {In daily life, humans often perform visual tasks, such as solving puzzles or searching for a friend in a crowd. Performing these visual searches jointly with a partner can be beneficial: The two task partners can devise effective division of labor strategies and thereby outperform individuals who search alone. To date, it is unknown whether these group benefits scale up to triads or whether the cost of coordinating with others offsets any potential benefit for group sizes above two. To address this question, we compare participants' performance in a visual search task that they perform either alone, in dyads, or in triads. When the search task is performed jointly, co-actors receive information about each other's gaze location. After controlling for speed\textendash accuracy trade-offs, we found that triads searched faster than dyads, suggesting that group benefits do scale up to triads. Moreover, we found that the triads' divided the search space in accordance with the co-actors' individual search performances but searched less efficiently than dyads. We also present a linear model to predict group benefits, which accounts for 70\% of the variance. The model includes our experimental factors and a set of non-redundant predictors, quantifying the similarities in the individual performances, the collaboration between co-actors, and the estimated benefits that co-actors would attain without collaborating. Overall, the present study demonstrates that group benefits scale up to larger group sizes, but the additional gains are attenuated by the increased costs associated with devising effective division of labor strategies.},
  file = {files/944/Wahn et al. - 2020 - Dyadic and triadic search Benefits, costs, and pr.pdf},
  journal = {Attention, Perception, \& Psychophysics},
  language = {en},
  number = {5}
}

@article{wahnPerformanceSimilaritiesPredict2018,
  title = {Performance Similarities Predict Collective Benefits in Dyadic and Triadic Joint Visual Search},
  author = {Wahn, Basil and Czeszumski, Artur and K{\"o}nig, Peter},
  editor = {Wan, Xiaoang},
  year = {2018},
  month = jan,
  volume = {13},
  pages = {e0191179},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0191179},
  abstract = {When humans perform tasks together, they may reach a higher performance in comparison to the best member of a group (i.e., a collective benefit). Earlier research showed that interindividual performance similarities predict collective benefits for several joint tasks. Yet, researchers did not test whether this is the case for joint visuospatial tasks. Also, researchers did not investigate whether dyads and triads reach a collective benefit when they are forbidden to exchange any information while performing a visuospatial task. In this study, participants performed a joint visual search task either alone, in dyads, or in triads, and were not allowed to exchange any information while doing the task. We found that dyads reached a collective benefit. Triads did outperform their best individual member and dyads\textemdash yet, they did not outperform the best dyad pairing within the triad. In addition, similarities in performance significantly predicted the collective benefit for dyads and triads. Furthermore, we find that the dyads' and triads' search performances closely match a simulated performance based on the individual search performances, which assumed that members of a group act independently. Overall, the present study supports the view that performance similarities predict collective benefits in joint tasks. Moreover, it provides a basis for future studies to investigate the benefits of exchanging information between co-actors in joint visual search tasks.},
  file = {files/943/Wahn et al. - 2018 - Performance similarities predict collective benefi.pdf},
  journal = {PLOS ONE},
  language = {en},
  number = {1}
}


